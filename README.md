# ğŸ§  Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions  
## An Insight into BiomedCLIP on Radiology

![BiomedCLIP](https://img.shields.io/badge/BiomedCLIP-Vision--Language--Model-blueviolet)  
![IU-Xray](https://img.shields.io/badge/Dataset-IU--Xray-lightgrey)  
![Status](https://img.shields.io/badge/Status-Research--In--Progress-yellow)

---

## ğŸ“Œ Abstract

This repository accompanies the study **"Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology"**, where we evaluate the performance and interpretability of **BiomedCLIP**, a vision-language model trained on biomedical data.

We pursue two main research objectives:
1. **Explore the embedding space** of BiomedCLIP to analyze meaningful class separations.
2. **Quantify limitations** of the model when applied to a highly imbalanced, out-of-distribution (OOD), multi-label medical dataset â€” **IU-Xray**.

We evaluate the model in three experimental settings:
- **Zero-shot inference**
- **Full fine-tuning**
- **Linear probing**

Our findings show that:
- In **zero-shot**, the model over-predicts all labels, leading to poor precision and weak class separability.
- **Full fine-tuning** improves discrimination between diseases.
- **Linear probing** enables detection of overlapping disease features.

Interpretability is enhanced through **Grad-CAM** visualizations, which are validated against **15 radiologist annotations**. These results call for **careful adaptation** of general VLMs to the specific characteristics of real-world medical datasets.

---

## ğŸ§ª Experiments & Dataset

- **Dataset**: [IU-Xray](https://pubmed.ncbi.nlm.nih.gov/26133894/)
  - Multi-label
  - High class imbalance
  - Small-scale
- **Model**: [BiomedCLIP](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224)
- **Evaluation Protocols**:
  - Zero-shot
  - Linear probing (frozen encoder)
  - Full fine-tuning (unfrozen encoder)

---

## ğŸ“Š Results Summary

**Overall evaluation metrics for BiomedCLIP under three settings on test set**  
(*`zs` = zero-shot, `ft` = fine-tuning, `lp` = linear probing*)  
ğŸ”µ **Blue = best**, ğŸ”´ **Red = worst**, â†‘/â†“ indicate desired direction

| Setting     | Inter-class Dist. â†‘ | Intra-class Dist. â†“ | Ratio â†‘ | F1 Score â†‘ | Exact Match â†‘ | LRAP â†‘ | Coverage Error â†“ | Time (min) â†“ |
|-------------|----------------------|-----------------------|----------|-------------|----------------|----------|---------------------|----------------|
| *zs*        | 31               | ğŸ”´21             | ğŸ”´ 1.5 | ğŸ”´ 0.1    | ğŸ”´ 0       | ğŸ”´0.3 | ğŸ”´7.7             | â€”              |
| *ft*        | ğŸ”´23          | ğŸ”µ13             | ğŸ”µ1.8 | ğŸ”µ0.2    | 0.1          | ğŸ”µ0.8 | ğŸ”µ2.6            | ğŸ”´15        |
| *lp*        | ğŸ”µ32            | 21               | 1.5    | 0.2       | ğŸ”µ0.1       | 0.7    | 3.1               | ğŸ”µ6.1           |

Grad-CAM results reveal differences in attention localization across the three modes, further aiding in interpretability.

---

## ğŸ“· Visualization Examples
![Grad-CAM Visualization](https://github.com/Nafiz95/BioVLM_Eval_CXR/blob/main/reduced_gradCXR.png)

---


---

## ğŸ› ï¸ Setup

```bash
# Clone the repo
git clone https://github.com/yourusername/Interpreting-BiomedCLIP.git
cd Interpreting-BiomedCLIP

# Run the ipynb code: Very Easy!
evaluatingBiomedClIP_IUXRAY.ipynb
```

---

## ğŸ“¢ Citation

Coming Soon On ArXiv
---

## ğŸ‘©â€âš•ï¸ Why This Matters

This work bridges **computer science** and **clinical relevance** by:
- Assessing the real-world **limitations of zero-shot VLMs** on small, imbalanced datasets.
- Stressing the **importance of interpretability** for deployment in healthcare.
- Offering insights into how models can mislead without explicit adaptation.

---

## ğŸ“¬ Contact

Have questions or feedback? Reach out to: **sadman.n@queensu.ca**
