# ğŸ§  Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions  
## An Insight into BiomedCLIP on Radiology

![BiomedCLIP](https://img.shields.io/badge/BiomedCLIP-Vision--Language--Model-blueviolet)  
![IU-Xray](https://img.shields.io/badge/Dataset-IU--Xray-lightgrey)  
![Status](https://img.shields.io/badge/Status-Research--In--Progress-yellow)

---

## ğŸ“Œ Abstract

This repository accompanies the study **"Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology"**, where we evaluate the performance and interpretability of **BiomedCLIP**, a vision-language model trained on biomedical data.

We pursue two main research objectives:
1. **Explore the embedding space** of BiomedCLIP to analyze meaningful class separations.
2. **Quantify limitations** of the model when applied to a highly imbalanced, out-of-distribution (OOD), multi-label medical dataset â€” **IU-Xray**.

We evaluate the model in three experimental settings:
- **Zero-shot inference**
- **Full fine-tuning**
- **Linear probing**

Our findings show that:
- In **zero-shot**, the model over-predicts all labels, leading to poor precision and weak class separability.
- **Full fine-tuning** improves discrimination between diseases.
- **Linear probing** enables detection of overlapping disease features.

Interpretability is enhanced through **Grad-CAM** visualizations, which are validated against **15 radiologist annotations**. These results call for **careful adaptation** of general VLMs to the specific characteristics of real-world medical datasets.

---

## ğŸ§ª Experiments & Dataset

- **Dataset**: [IU-Xray](https://pubmed.ncbi.nlm.nih.gov/26133894/)
  - Multi-label
  - High class imbalance
  - Small-scale
- **Model**: [BiomedCLIP](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224)
- **Evaluation Protocols**:
  - Zero-shot
  - Linear probing (frozen encoder)
  - Full fine-tuning (unfrozen encoder)

---

## ğŸ“Š Results Summary

**Overall evaluation metrics for BiomedCLIP under three settings on test set**  
(*`zs` = zero-shot, `ft` = fine-tuning, `lp` = linear probing*)  
ğŸ”µ **Blue = best**, ğŸ”´ **Red = worst**, â†‘/â†“ indicate desired direction

| Setting     | Inter-class Dist. â†‘ | Intra-class Dist. â†“ | Ratio â†‘ | F1 Score â†‘ | Exact Match â†‘ | LRAP â†‘ | Coverage Error â†“ | Time (min) â†“ |
|-------------|----------------------|-----------------------|----------|-------------|----------------|----------|---------------------|----------------|
| *zs*        | 31.890               | ğŸ”´ 21.160             | ğŸ”´ 1.506 | ğŸ”´ 0.105    | ğŸ”´ 0.000       | ğŸ”´ 0.250 | ğŸ”´ 7.70             | â€”              |
| *ft*        | ğŸ”´ 22.754            | ğŸ”µ 12.756             | ğŸ”µ 1.784 | ğŸ”µ 0.235    | 0.134          | ğŸ”µ 0.779 | ğŸ”µ 2.750            | ğŸ”´ 15.47        |
| *lp*        | ğŸ”µ 31.894            | 21.160               | 1.507    | 0.183       | ğŸ”µ 0.143       | 0.741    | 3.077               | 6.10           |

Grad-CAM results reveal differences in attention localization across the three modes, further aiding in interpretability.

---

## ğŸ“· Visualization Examples

- âœ… Visual heatmaps using **Grad-CAM**
- ğŸ“Œ Layer-wise attention comparisons
- ğŸ” Side-by-side with radiologist annotations

---

## ğŸ“ Repository Structure

```bash
ğŸ“¦ Interpreting-BiomedCLIP/
â”œâ”€â”€ data/                    # IU-Xray dataset setup
â”œâ”€â”€ models/                  # BiomedCLIP loading & config
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ zero_shot/
â”‚   â”œâ”€â”€ linear_probe/
â”‚   â””â”€â”€ fine_tune/
â”œâ”€â”€ visualizations/         # Grad-CAM outputs and scripts
â”œâ”€â”€ notebooks/              # Jupyter notebooks for analysis
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup

```bash
# Clone the repo
git clone https://github.com/yourusername/Interpreting-BiomedCLIP.git
cd Interpreting-BiomedCLIP

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“¢ Citation

If you use this work or the codebase, please cite:

```bibtex
@misc{yourbib2025,
  title={Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology},
  author={Your Name et al.},
  year={2025},
  note={Work in progress},
}
```

---

## ğŸ‘©â€âš•ï¸ Why This Matters

This work bridges **computer science** and **clinical relevance** by:
- Assessing the real-world **limitations of zero-shot VLMs** on small, imbalanced datasets.
- Stressing the **importance of interpretability** for deployment in healthcare.
- Offering insights into how models can mislead without explicit adaptation.

---

## ğŸ“¬ Contact

Have questions or feedback? Reach out to: **your.email@domain.com**
